{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_final_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivS4gGjhItlG"
      },
      "source": [
        "# Install Newspaper3k Library\n",
        "!pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAHdTnzSIyuj"
      },
      "source": [
        "# Imports for Newspaper3k Library\n",
        "import pandas as pd\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmK78EpoJPCd"
      },
      "source": [
        "## Scraping multiple urls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGu1XGz2I6t3"
      },
      "source": [
        "# Define list of urls\n",
        "list_of_urls = ['https://ellenmacarthurfoundation.org/topics/plastics/latest-content',\n",
        "                'https://www.nytimes.com/search?dropmab=false&query=ocean%20plastic&sort=newest',\n",
        "                'https://www.worldwildlife.org/search?cx=003443374396369277624%3Av3nraqhmeyk&ie=UTF-8&x=ocean+plastic&cat=stories#gsc.tab=0&gsc.q=[…]ic&gsc.sort=date',\n",
        "                'https://news.un.org/en/search/ocean%20plastic', 'https://oceanconservancy.org/newsroom/press-releases/?_search=ocean%20plastic', \n",
        "                'https://www.linkedin.com/search/results/content/?keywords=ocean%20plastic&origin=SWITCH_SEARCH_VERTICAL&sid=FrD']\n",
        "  \n",
        "# Parse through each url and display its content\n",
        "top_articles = []\n",
        "\n",
        "for url in list_of_urls:\n",
        "    url_site = newspaper.build(url=\"%s\" % (url), language='en', memoize_articles=False)\n",
        "    \n",
        "    # Loop through top 5 articles on each websites\n",
        "    for i in range(0,5):\n",
        "\n",
        "    # Create dictionary to hold title, summary, and url from for loop \n",
        "      news = {}\n",
        "      article = url_site.articles[i]\n",
        "      article.download()\n",
        "      article.parse()\n",
        "      article.nlp()\n",
        "      top_articles.append(article)\n",
        "      news['title'] = article.title\n",
        "      news['summary']= article.summary\n",
        "      news['url'] = article.url\n",
        "      news['pic'] = article.top_image\n",
        "      news['source'] = article.authors\n",
        "      news['date'] = article.publish_date\n",
        "      top_articles.append(news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr0P0PaXJU3_"
      },
      "source": [
        "## Scraping individual urls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-lMFdBzJoXu"
      },
      "source": [
        "### WWF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhjEggThJXxF"
      },
      "source": [
        "# WWF\n",
        "# Define site\n",
        "site = newspaper.build(\"https://www.worldwildlife.org/search?cx=003443374396369277624%3Av3nraqhmeyk&ie=UTF-8&x=ocean+plastic&cat=stories#gsc.tab=0&gsc.q=[…]ic&gsc.sort=date\", memoize_articles=False)  \n",
        "# Create list to hold top article info.\n",
        "top_wwf_articles = []\n",
        "\n",
        "for i in range(0,5):\n",
        "    # Create dictionary to hold title, summary, and url from for loop \n",
        "    news = {}\n",
        "    article = site.articles[i]\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "    top_wwf_articles.append(article)\n",
        "    news['title'] = article.title\n",
        "    news['summary']= article.summary\n",
        "    news['url'] = article.url\n",
        "    news['pic'] = article.top_image\n",
        "    news['source'] = article.authors\n",
        "    news['date'] = article.publish_date  \n",
        "    top_wwf_articles.append(news)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXC7i2m9L6AZ"
      },
      "source": [
        "# Print articles\n",
        "print(top_wwf_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXrcpH2rJqsp"
      },
      "source": [
        "### NYT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x44xur4dJnmQ"
      },
      "source": [
        "# NYT \n",
        "\n",
        "# Get links to Articles on NYT\n",
        "nyt_site = newspaper.build('https://www.nytimes.com/search?dropmab=false&query=ocean%20plastic&sort=newest', memoize_articles=False)  \n",
        " \n",
        "# Create list of top 5 articles from NYT \n",
        "  \n",
        "top_nyt_articles = []\n",
        "\n",
        "for i in range(0,5):\n",
        "    # Create dictionary to hold title, summary, and url from for loop \n",
        "    news = {}\n",
        "    article = nyt_site.articles[i]\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "    top_nyt_articles.append(article)\n",
        "    news['title'] = article.title\n",
        "    news['summary']= article.summary\n",
        "    news['url'] = article.url\n",
        "    news['pic'] = article.top_image\n",
        "    news['source'] = article.authors\n",
        "    news['date'] = article.publish_date\n",
        "    top_nyt_articles.append(news)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq-hYkqoL-wn"
      },
      "source": [
        "# Print articles\n",
        "print(top_nyt_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoRkY9rtJzfm"
      },
      "source": [
        "### EMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5enZxN_zJzzQ"
      },
      "source": [
        "# Ellen Macarthur Foundation\n",
        "# Define site\n",
        "site = newspaper.build(\"https://ellenmacarthurfoundation.org/topics/plastics/latest-content\", memoize_articles=False)  \n",
        "# Create list to hold top article info.\n",
        "top_emf_articles = []\n",
        "\n",
        "for i in range(0,5):\n",
        "    # Create dictionary to hold title, summary, and url from for loop \n",
        "    news = {}\n",
        "    article = site.articles[i]\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "    top_emf_articles.append(article)\n",
        "    news['title'] = article.title\n",
        "    news['summary']= article.summary\n",
        "    news['url'] = article.url\n",
        "    news['pic'] = article.top_image\n",
        "    news['source'] = article.authors\n",
        "    news['date'] = article.publish_date  \n",
        "    top_emf_articles.append(news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5oePHnhMIRi"
      },
      "source": [
        "# Print articles\n",
        "print(top_emf_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ikiTzMGKqGV"
      },
      "source": [
        "### UN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPhss26XKgM5"
      },
      "source": [
        "# UN\n",
        "# Define site\n",
        "site = newspaper.build('https://news.un.org/en/search/ocean%20plastic', memoize_articles=False)  \n",
        "# Create list to hold top article info.\n",
        "top_un_articles = []\n",
        "\n",
        "for i in range(0,5):\n",
        "    # Create dictionary to hold title, summary, and url from for loop \n",
        "    news = {}\n",
        "    article = site.articles[i]\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "    top_un_articles.append(article)\n",
        "    news['title'] = article.title\n",
        "    news['summary']= article.summary\n",
        "    news['url'] = article.url\n",
        "    news['pic'] = article.top_image\n",
        "    news['source'] = article.authors\n",
        "    news['date'] = article.publish_date  \n",
        "    top_un_articles.append(news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA-263ZHML-v"
      },
      "source": [
        "# Print articles\n",
        "print(top_un_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYBFAblzLKtO"
      },
      "source": [
        "### Ocean Conservancy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuuxXKVwKxIQ"
      },
      "source": [
        "\n",
        "# Ocean Conservancy\n",
        "# Define site\n",
        "site = newspaper.build('https://oceanconservancy.org/newsroom/press-releases/?_search=ocean%20plastic', memoize_articles=False)  \n",
        "# Create list to hold top article info.\n",
        "top_oc_articles = []\n",
        "\n",
        "for i in range(0,5):\n",
        "    # Create dictionary to hold title, summary, and url from for loop \n",
        "    news = {}\n",
        "    article = site.articles[i]\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "    top_oc_articles.append(article)\n",
        "    news['title'] = article.title\n",
        "    news['summary']= article.summary\n",
        "    news['url'] = article.url\n",
        "    news['pic'] = article.top_image\n",
        "    news['source'] = article.authors\n",
        "    news['date'] = article.publish_date  \n",
        "    top_oc_articles.append(news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezwz2dz8MQDh"
      },
      "source": [
        "# Print articles\n",
        "print(top_oc_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrP3WBSgLI19"
      },
      "source": [
        "### LinkedIn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNoyUmNIK8OH"
      },
      "source": [
        "# LinkedIn\n",
        "# Define site\n",
        "site = newspaper.build('https://www.linkedin.com/search/results/content/?keywords=ocean%20plastic&origin=SWITCH_SEARCH_VERTICAL&sid=FrD', memoize_articles=False)  \n",
        "# Create list to hold top article info.\n",
        "top_li_articles = []\n",
        "\n",
        "for i in range(0,5):\n",
        "    # Create dictionary to hold title, summary, and url from for loop \n",
        "    news = {}\n",
        "    article = site.articles[i]\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "    top_li_articles.append(article)\n",
        "    news['title'] = article.title\n",
        "    news['summary']= article.summary\n",
        "    news['url'] = article.url\n",
        "    news['pic'] = article.top_image\n",
        "    news['source'] = article.authors\n",
        "    news['date'] = article.publish_date  \n",
        "    top_li_articles.append(news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWYapPkTMS3b"
      },
      "source": [
        "# Print articles\n",
        "print(top_li_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGnEv32bMejX"
      },
      "source": [
        "## Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-8dbm7GMnWa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhqnMDXEMn1X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbCwlQK3MnxP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-ylej8Mnsv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}